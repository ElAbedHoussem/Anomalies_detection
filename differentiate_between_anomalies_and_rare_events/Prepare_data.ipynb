{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "seed = 32\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "target =  'APP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ELHMA data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def read_and_reformate_data(path):\\n    original_data = pd.read_csv(path)\\n     ### derive PLV_Totale and drop PLV of all stations\\n    original_data['PLV'] = original_data.iloc[:,4:24].sum(axis=1)    \\n    original_data.drop(original_data.iloc[:,4:23], axis=1, inplace=True)\\n    original_data.set_index('DATE', inplace=True)\\n    original_data.to_csv('hma_full_data.csv',index=False, sep=',')\\n    return original_data\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def read_and_reformate_data(path):\n",
    "    original_data = pd.read_csv(path)\n",
    "     ### derive PLV_Totale and drop PLV of all stations\n",
    "    original_data['PLV'] = original_data.iloc[:,4:24].sum(axis=1)    \n",
    "    original_data.drop(original_data.iloc[:,4:23], axis=1, inplace=True)\n",
    "    original_data.set_index('DATE', inplace=True)\n",
    "    original_data.to_csv('hma_full_data.csv',index=False, sep=',')\n",
    "    return original_data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"elhma_data = read_and_reformate_data('full_dataset.csv')\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"elhma_data = read_and_reformate_data('full_dataset.csv')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Kelibia climatic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def read_and_reformate_climatic_data(path):\n",
    "    df = pd.read_excel(path, sheet_name='KELIBIA')\n",
    "    df = df.drop(['NUM_STA','NOM_STA','Unnamed: 7'],axis=1)\n",
    "    df.rename(columns={'DAT': 'DATE', 'Temp. Minimale °C' : 'Temp_min', 'Temp. Maximale °C':'Temp_max',\n",
    "                        'Précipitation toat (mm)':'PLV', 'Evaporation Pch (mm)':'EVA'},inplace=True)\n",
    "    df['DATE'] = pd.DatetimeIndex(df['DATE'])   \n",
    "    df.set_index('DATE', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelibia_climatic_data = read_and_reformate_climatic_data('KELIBIA.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Kelibia inflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inflow target\n",
    "def read_and_reformate_inflow(file):\n",
    "    #convert string french months to numeric values\n",
    "    df = pd.read_excel(file, sheet_name='Sheet1')\n",
    "    df.rename(columns={'apport':'APP'},inplace=True)\n",
    "    dates = df.pop('date')\n",
    "    dates = dates.str.split(' ')\n",
    "    months_names = ['Janvier','Février','Mars','Avril','Mai','Juin','Juillet','Aôut','Septembre','Octobre','Novembre','Décembre']\n",
    "    months_values = ['01/01','02/01','03/01','04/01','05/01','06/01','07/01','08/01','09/01','10/01','11/01','12/01']\n",
    "    data_date_format = []\n",
    "    for date in dates:\n",
    "        index = months_names.index(date[0])\n",
    "        data_date_format.append(months_values[index]+'/'+date[1])\n",
    "    df['date']= pd.DatetimeIndex(data_date_format)  \n",
    "    df.set_index('date', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kelibia_inflow_data = read_and_reformate_inflow('apport.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate kelibia climatic data & inflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def concatunate_inflow(data_cleaned, inflow): \n",
    "    data = data_cleaned.copy()\n",
    "    data['APP'] = np.nan\n",
    "    for year in sorted(pd.unique(pd.DatetimeIndex(inflow.index).year)):\n",
    "        for month in range(1,13):\n",
    "            index_app = list(inflow[np.logical_and(year==pd.DatetimeIndex(inflow.index).year, \n",
    "                                                   month==pd.DatetimeIndex(inflow.index).month)].index)\n",
    "            index_data = list(data[np.logical_and(year==pd.DatetimeIndex(data.index).year, \n",
    "                                               month==pd.DatetimeIndex(data.index).month)].index)\n",
    "            if len(index_data)>0 and len(index_app)>0:\n",
    "                data.loc[index_data, 'APP'] = inflow.loc[index_app, 'APP'].values[0]   \n",
    "    data.dropna(axis=0,inplace=True)\n",
    "    return data\n",
    "\n",
    "kelibia_data = concatunate_inflow(kelibia_climatic_data, kelibia_inflow_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create statistical data for Kelibia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistic_data(full_data):\n",
    "    new_data = pd.DataFrame()\n",
    "    #For each year\n",
    "    full_data['DATE'] = full_data.index\n",
    "    for year in pd.unique(pd.DatetimeIndex(full_data['DATE']).year):\n",
    "        local_year_df = full_data[pd.DatetimeIndex(full_data['DATE']).year == year]\n",
    "        #for each month\n",
    "        for month in pd.unique(pd.DatetimeIndex(local_year_df['DATE']).month):\n",
    "            #Get all samples for each month\n",
    "            KELIBIA = full_data[(pd.DatetimeIndex(full_data['DATE']).month== month) & (pd.DatetimeIndex(full_data['DATE']).year ==year)]\n",
    "            date = KELIBIA[['DATE']]\n",
    "            # get min, max, mean, std, 25%, 50% and 75% for each column\n",
    "            KELIBIA = KELIBIA.describe().drop(['count'],axis=0)\n",
    "            #convert the DF from N*M dimonssion to 1*(M*N)  dimonssion\n",
    "            KELIBIA = KELIBIA.unstack().to_frame().T\n",
    "            KELIBIA.columns = ['_'.join(column) for column in KELIBIA.columns]\n",
    "            #set the first date to meet as index \n",
    "            KELIBIA['DATE'] = date.iloc[0][0]\n",
    "            KELIBIA = KELIBIA.set_index(KELIBIA['DATE'])\n",
    "\n",
    "            new_data = new_data.append(KELIBIA)\n",
    "    new_data = new_data.drop(['DATE','APP_std','APP_min','APP_25%','APP_50%','APP_75%','APP_max'],axis=1)\n",
    "    new_data.rename(columns={'APP_mean':'APP'},inplace=True)\n",
    "\n",
    "    return new_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kelibia_statistical_data = create_statistic_data(kelibia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_fo_missing_values(full_data):\n",
    "    data = full_data\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (100*data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data[missing_data.Percent>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Total, Percent]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_percent_fo_missing_values(kelibia_statistical_data)\n",
    "get_percent_fo_missing_values(kelibia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_percent_fo_missing_values(elhma_data)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"get_percent_fo_missing_values(elhma_data)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(original_data):\n",
    "    clean_data = original_data.copy()\n",
    "    clean_data.drop_duplicates(keep='first', inplace=True)\n",
    "    ### fill missing data \n",
    "    clean_data.interpolate(inplace=True)\n",
    "    clean_data.fillna(method='bfill',inplace=True)    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elhma_data = preprocessing(elhma_data)'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"elhma_data = preprocessing(elhma_data)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kelibia_statistical_data = preprocessing(kelibia_statistical_data)\n",
    "kelibia_data = preprocessing(kelibia_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split & Scaling & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data_into_train_test_sets(ts_features_targets):\n",
    "    X = ts_features_targets.drop('APP', axis=1)\n",
    "    y = ts_features_targets[['APP']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=seed, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling and transform data\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "def data_scaling_transform(X_train, X_test, y_train, y_test):\n",
    "    ### transform input variables\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    ### transform target variables\n",
    "    transformer = PowerTransformer().fit(y_train)\n",
    "    y_train = transformer.transform(y_train)\n",
    "    y_test = transformer.transform(y_test)     \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kelibia_statistical_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2d36f94f1e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data_into_train_test_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkelibia_statistical_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_scaling_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kelibia_statistical_data' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data_into_train_test_sets(kelibia_statistical_data)\n",
    "X_train, X_test, y_train, y_test = data_scaling_transform(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_test = split_data_into_train_test_sets(ts_features_targets)\n",
    "X_train, X_test, y_train, y_test = data_scaling_transform(X_train, X_test, y_train, y_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kelibia_statistical_data.to_csv('kelibia_statistical_data.csv',index=False, sep=',')\n",
    "kelibia_data.to_csv('kelibia_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermanagement",
   "language": "python",
   "name": "watermanagement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
