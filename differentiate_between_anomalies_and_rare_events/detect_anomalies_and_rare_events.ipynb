{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Input,Dropout,Embedding,LSTM\n",
    "from keras.optimizers import RMSprop,Adam,Nadam\n",
    "from keras.preprocessing import sequence \n",
    "from keras.callbacks import TensorBoard,CSVLogger\n",
    "from keras.utils import plot_model\n",
    "seed = 32\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "##from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle \n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import tensorflow\n",
    "import time \n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kelibia_data.csv')\n",
    "df = df.set_index(df['DATE']).drop(['DATE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temp_min</th>\n",
       "      <th>Temp_max</th>\n",
       "      <th>PLV</th>\n",
       "      <th>EVA</th>\n",
       "      <th>APP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-08-02</th>\n",
       "      <td>24.5</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-09-02</th>\n",
       "      <td>20.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-09-10</th>\n",
       "      <td>21.3</td>\n",
       "      <td>28.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-09-21</th>\n",
       "      <td>24.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-09-25</th>\n",
       "      <td>19.3</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-17</th>\n",
       "      <td>8.6</td>\n",
       "      <td>13.5</td>\n",
       "      <td>19.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-24</th>\n",
       "      <td>14.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-27</th>\n",
       "      <td>16.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-29</th>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-11-30</th>\n",
       "      <td>10.4</td>\n",
       "      <td>18.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2172 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Temp_min  Temp_max   PLV  EVA    APP\n",
       "DATE                                            \n",
       "1986-08-02      24.5      29.4   0.0  3.3  0.000\n",
       "1986-09-02      20.5      25.5  24.5  3.2  0.000\n",
       "1986-09-10      21.3      28.2   0.8  2.9  0.000\n",
       "1986-09-21      24.0      28.5   3.5  5.6  0.000\n",
       "1986-09-25      19.3      26.6   0.5  2.5  0.000\n",
       "...              ...       ...   ...  ...    ...\n",
       "2007-11-17       8.6      13.5  19.6  2.0  3.588\n",
       "2007-11-24      14.8      21.7   0.2  1.4  3.588\n",
       "2007-11-27      16.0      20.3   0.4  1.3  3.588\n",
       "2007-11-29      12.0      19.0   5.8  0.9  3.588\n",
       "2007-11-30      10.4      18.2   2.9  1.5  3.588\n",
       "\n",
       "[2172 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_df(df):\n",
    "    dic={}\n",
    "    for c in df.columns :\n",
    "        if (df[c].dtype ==\"object\"):\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(df[c])\n",
    "            df[c]=encoder.transform(df[c])\n",
    "            dic[c]=encoder\n",
    "            print('ok')\n",
    "    with open(\"./models/LabelEncoders_dic.pickle\",\"wb\") as f:\n",
    "        pickle.dump(dic,f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataframe(df):\n",
    "    df = shuffle(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliser_all_columns(df):\n",
    "    diction={}\n",
    "    \n",
    "    for c in df.columns :\n",
    "        scaler=MinMaxScaler(feature_range=(0,1)).fit(df[c].values.reshape(-1,1))\n",
    "        diction[c]=scaler        \n",
    "        df[c]=scaler.transform(df[c].values.reshape(-1,1))\n",
    "    with open(\"./models/MinMaxScalers_dic.pickle\",\"wb\") as f:\n",
    "        pickle.dump(diction,f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=transformer_df(df)\n",
    "df=normaliser_all_columns(df)\n",
    "df=shuffle_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data_into_train_test_sets(ts_features_targets):\n",
    "    X = ts_features_targets.drop('APP', axis=1)\n",
    "    y = ts_features_targets[['APP']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=seed, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('APP', axis=1)\n",
    "y = df[['APP']]\n",
    "X_train, X_test, y_train, y_test = split_data_into_train_test_sets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling and transform data\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "def data_scaling_transform(X_train= None, X_test= None, y_train= None, y_test= None, X= None, y= None, splited_data = True):\n",
    "    ### transform input variables\n",
    "    if (splited_data == True):\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "        ### transform target variables\n",
    "        transformer = PowerTransformer().fit(y_train)\n",
    "        y_train = transformer.transform(y_train)\n",
    "        y_test = transformer.transform(y_test)     \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        scaler = StandardScaler().fit(X)\n",
    "        X = pd.DataFrame(scaler.transform(X), columns=X.columns, index=X.index)\n",
    "        transformer = PowerTransformer().fit(y)\n",
    "        y = transformer.transform(y)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data_scaling_transform(None, None, None, None, X, y, False)\n",
    "X_train, X_test, y_train, y_test = data_scaling_transform(X_train, X_test, y_train, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ae(input_dim):\n",
    "    \n",
    "    dae_input=Input(shape=(input_dim,)) #Define the input layer\n",
    "    \n",
    "    encoded= Dense(units=input_dim,activation=\"tanh\")(dae_input) \n",
    "    encoded= Dense(units=int(input_dim),activation=\"tanh\")(encoded) \n",
    "    encoded= Dense(units=int(input_dim),activation=\"tanh\")(encoded)\n",
    "    encoded= Dense(units=int(input_dim/6),activation=\"tanh\")(encoded)\n",
    "    encoded= Dense(units=3,activation=\"tanh\")(encoded)\n",
    "    encoded= Dense(units=int(input_dim/6),activation=\"tanh\")(encoded)\n",
    "    decoded= Dense(units=int(input_dim/4),activation=\"tanh\")(encoded)\n",
    "    decoded= Dense(units=int(input_dim/2),activation=\"tanh\")(decoded)\n",
    "    decoded= Dense(units=input_dim,activation=\"tanh\")(decoded)\n",
    "    decoded= Dense(units=input_dim,activation=\"linear\",name='decoded')(decoded)\n",
    "    \n",
    "    model = Model(inputs=dae_input, outputs=decoded)\n",
    "    model.compile(optimizer=RMSprop(),loss=\"mean_squared_error\",metrics=[\"mae\"])\n",
    "    model.summary()\n",
    "    plot_model(model,to_file='ae.png',show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras import Model\\nfrom keras.layers import Input, Dense\\nfrom keras.optimizers import RMSprop\\n\\ndef create_ae(input_dim, latent_dim):\\n    \\n    input_layer=Input(shape=(input_dim,)) #Define the input layer\\n    \\n    encoded= Dense(units=input_dim,activation=\"tanh\")(input_layer) \\n    encoded= Dense(units=3,activation=\"tanh\")(encoded) \\n    \\n    laten_vector= Dense(units=latent_dim,activation=\"tanh\")(encoded)\\n\\n    decoded= Dense(units=3,activation=\"tanh\")(laten_vector)\\n    decoded= Dense(units=input_dim,activation=\"tanh\")(decoded)\\n    \\n    output_layer= Dense(units=input_dim,activation=\"linear\",name=\\'decoded\\')(decoded)\\n    \\n    model = Model(inputs=input_layer, outputs=output_layer)\\n    model.compile(optimizer=RMSprop(),loss=\"mean_squared_error\",metrics=[\"mae\"])\\n    model.summary()\\n    #plot_model(model,to_file=\\'ae.png\\',show_shapes=True)\\n    return model'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from keras import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "def create_ae(input_dim, latent_dim):\n",
    "    \n",
    "    input_layer=Input(shape=(input_dim,)) #Define the input layer\n",
    "    \n",
    "    encoded= Dense(units=input_dim,activation=\"tanh\")(input_layer) \n",
    "    encoded= Dense(units=3,activation=\"tanh\")(encoded) \n",
    "    \n",
    "    laten_vector= Dense(units=latent_dim,activation=\"tanh\")(encoded)\n",
    "\n",
    "    decoded= Dense(units=3,activation=\"tanh\")(laten_vector)\n",
    "    decoded= Dense(units=input_dim,activation=\"tanh\")(decoded)\n",
    "    \n",
    "    output_layer= Dense(units=input_dim,activation=\"linear\",name='decoded')(decoded)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=RMSprop(),loss=\"mean_squared_error\",metrics=[\"mae\"])\n",
    "    model.summary()\n",
    "    #plot_model(model,to_file='ae.png',show_shapes=True)\n",
    "    return model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-59649cac2b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautoecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-23e2f139ba7d>\u001b[0m in \u001b[0;36mcreate_ae\u001b[0;34m(input_dim)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mencoded\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdecoded\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m             if all([s is not None\n\u001b[1;32m    505\u001b[0m                     for s in to_list(input_shape)]):\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoecoder = create_ae(X_train.shape[1])#, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoecoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f3a04b7120f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m autoecoder.fit(X_train,X_train,\n\u001b[0m\u001b[1;32m      5\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoecoder' is not defined"
     ]
    }
   ],
   "source": [
    "batchsize=100\n",
    "epoch=10\n",
    "start_time = time.time() \n",
    "autoecoder.fit(X_train,X_train,\n",
    "              batch_size=batchsize,\n",
    "              epochs=epoch,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=(X_test,X_test),\n",
    "              callbacks=[TensorBoard(log_dir=\"./logs/autoencoder\")])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {\"./logs/autoencoder/\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the avrage length distribution of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal_data=df.drop(\"APP\",axis=1)\n",
    "y_pred=autoecoder.predict(X)\n",
    "y_dist=np.linalg.norm(X-y_pred,axis=-1)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(y_dist,bins=200)\n",
    "plt.xlabel(\"Normal Data Average Loss Length\")\n",
    "plt.ylabel(\"Number of data points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "y_pred=autoecoder.predict(X_test) # predict new data \n",
    "y_dist=np.linalg.norm(X_test-y_pred,axis=-1) # calculate distance between real  and predicted data\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(y_dist,bins=200)\n",
    "plt.xlabel(\"Anomaly DataAverage Loss Length\")\n",
    "plt.ylabel(\"Number of data points\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## we see that all the anamalies data are more then 1.5 average length loss\n",
    "## as a conclusion we will make the 1.5 as our average loss threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(xtest, threshold_anomalies, percent_of_anoamlies):\n",
    "    start_time = time.time() \n",
    "    y_pred=autoecoder.predict(xtest) \n",
    "    var_inter = xtest-y_pred\n",
    "    y_dist=np.linalg.norm(var_inter,axis=-1) \n",
    "    result = []\n",
    "    for index, elem in enumerate(y_dist):\n",
    "        if elem >threshold_anomalies:\n",
    "            values_more_then_the_limit = 0\n",
    "            l= len(var_inter.columns)\n",
    "            for item in var_inter.iloc[index]:\n",
    "                if item > percent_of_anoamlies:\n",
    "                    values_more_then_the_limit+=1\n",
    "            if(values_more_then_the_limit/l)>=percent_of_anoamlies:\n",
    "                result.append(('Element number {0} is a rare event'.format(index), str(round(values_more_then_the_limit/l,2))))\n",
    "            else:\n",
    "                result.append(('Element number {0} is an anomaly'.format(index), str(round(values_more_then_the_limit/l,2))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred=prediction(X_test, 5, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_with_statistical_data(xtest, threshold_anomalies, percent_of_anoamlies):\n",
    "    cleaned_df = pd.DataFrame()\n",
    "    y_pred=autoecoder.predict(xtest) \n",
    "    var_inter = xtest-y_pred\n",
    "    y_dist=np.linalg.norm(var_inter,axis=-1) \n",
    "    string_result = []\n",
    "    values_result = []\n",
    "    for index, elem in enumerate(y_dist):\n",
    "        features = {'Temp_min':0, 'Temp_max':0, 'PLV':0, 'EVA':0, \"index\":index}\n",
    "        if elem >threshold_anomalies:\n",
    "            values_more_then_the_limit = 0\n",
    "            l= len(var_inter.columns)\n",
    "            for idx, item in enumerate(var_inter.iloc[index]):\n",
    "                if item > percent_of_anoamlies:\n",
    "                    #values_more_then_the_limit+=1\n",
    "                    if(idx in range(0,7)):\n",
    "                        features['Temp_min'] = 1\n",
    "                    if(idx in range(7,14)):\n",
    "                        features['Temp_max'] = 1\n",
    "                    if(idx in range(14,21)):\n",
    "                        features['PLV'] = 1\n",
    "                    if(idx in range(21,28)):\n",
    "                        features['EVA'] = 1\n",
    "            values_result.append(features)\n",
    "            if features['Temp_min']==1 and features['Temp_max']==1 and features['PLV']==1 and features['EVA']==1:\n",
    "                string_result.append('Element number {0} is a rare event'.format(features['index']))\n",
    "            else:\n",
    "                string_result.append('Element number {0} is an anomaly'.format(features['index']))\n",
    "        else:\n",
    "            cleaned_df = cleaned_df.append(var_inter.iloc[index])\n",
    "                                                                               \n",
    "    '''        \n",
    "    if(values_more_then_the_limit/l)>=percent_of_anoamlies:\n",
    "        result.append(('Element number {0} is a rare event'.format(index), str(round(values_more_then_the_limit/l,2))))\n",
    "    else:\n",
    "        result.append(('Element number {0} is an anomaly'.format(index), str(round(values_more_then_the_limit/l,2))))\n",
    "    '''\n",
    "    return values_result, string_result, cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values_result, string_result, cleaned_df=predictions_with_statistical_data(X, 5, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_with_original_data(X, threshold_anomalies, max_gap_between_real_predicted_data, percent_of_anoamlies):\n",
    "    cleaned_df = pd.DataFrame()\n",
    "    y_pred=autoecoder.predict(X) \n",
    "    var_inter = X-y_pred\n",
    "    y_dist=np.linalg.norm(var_inter,axis=-1) \n",
    "    string_result = []\n",
    "    values_result = []\n",
    "    for index, elem in enumerate(y_dist):\n",
    "        features = {'Temp_min':0, 'Temp_max':0, 'PLV':0, 'EVA':0, \"index\":index}\n",
    "        if elem >threshold_anomalies:\n",
    "            values_more_then_the_limit = 0\n",
    "            #l= len(var_inter.columns)\n",
    "            for idx, item in enumerate(var_inter.iloc[index]):\n",
    "                if (item > max_gap_between_real_predicted_data or item < -max_gap_between_real_predicted_data):\n",
    "                    #values_more_then_the_limit+=1\n",
    "                    if idx == 0 :\n",
    "                        features['Temp_min'] = 1\n",
    "                    if idx == 1 :\n",
    "                        features['Temp_max'] = 1\n",
    "                    if idx == 2 :\n",
    "                        features['PLV'] = 1\n",
    "                    if idx ==3:\n",
    "                        features['EVA'] = 1\n",
    "                    values_more_then_the_limit+= 1 \n",
    "            if(values_more_then_the_limit/len(var_inter.iloc[index])> percent_of_anoamlies):\n",
    "                print('this is a rare event')\n",
    "            values_result.append(features)\n",
    "            if features['Temp_min']==1 and features['Temp_max']==1 and features['PLV']==1 and features['EVA']==1:\n",
    "                string_result.append('Element number {0} is a rare event'.format(features['index']))\n",
    "            else:\n",
    "                string_result.append('Element number {0} is an anomaly'.format(features['index']))\n",
    "                \n",
    "            cleaned_df[X.iloc[index].name] = [np.nan, np.nan, np.nan, np.nan]\n",
    "        else:\n",
    "            cleaned_df[X.iloc[index].name] = X.iloc[index]\n",
    "                                                                               \n",
    "    '''        \n",
    "    if(values_more_then_the_limit/l)>=percent_of_anoamlies:\n",
    "        result.append(('Element number {0} is a rare event'.format(index), str(round(values_more_then_the_limit/l,2))))\n",
    "    else:\n",
    "        result.append(('Element number {0} is an anomaly'.format(index), str(round(values_more_then_the_limit/l,2))))\n",
    "    '''\n",
    "    return values_result, string_result, cleaned_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_anomalies = 5\n",
    "max_gap_between_real_predicted_data = 2.5 \n",
    "percent_of_anoamlies = 0.7\n",
    "values_result, string_result, cleaned_df=predictions_with_original_data(X, threshold_anomalies,\n",
    "                                                                        max_gap_between_real_predicted_data,\n",
    "                                                                        percent_of_anoamlies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoecoder.save(\"./models/ae.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.merge(cleaned_df, df[['APP']], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7af064d4cdcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DATE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#cleaned_df['DATE'] = cleaned_df.index.get_level_values('DATE')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./cleaned_data.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_df' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_df.index.name = 'DATE'\n",
    "#cleaned_df['DATE'] = cleaned_df.index.get_level_values('DATE') \n",
    "cleaned_df.to_excel('./cleaned_data.xlsx', index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('./cleaned_data.xlsx', index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/houssem/Téléchargements/RAPPORT DE TRAVAIL DU MOIS JUIN.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
