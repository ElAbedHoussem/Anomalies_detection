{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 32\n",
    "CLEANED_DATA = \"cleaned_data.xlsx\"\n",
    "cleaned_data_with_AEGMM= 'cleaned_data_with_AEGMM.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I.Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts_features_targets = pd.read_excel(CLEANED_DATA)\n",
    "ts_features_targets = pd.read_csv(cleaned_data_with_AEGMM)\n",
    "ts_features_targets = ts_features_targets.set_index(ts_features_targets['DATE']).drop(['DATE'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1970"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts_features_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create statistical data for Kelibia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistic_data(full_data):\n",
    "    new_data = pd.DataFrame()\n",
    "    #For each year\n",
    "    full_data['DATE'] = full_data.index\n",
    "    for year in pd.unique(pd.DatetimeIndex(full_data['DATE']).year):\n",
    "        local_year_df = full_data[pd.DatetimeIndex(full_data['DATE']).year == year]\n",
    "        #for each month\n",
    "        for month in pd.unique(pd.DatetimeIndex(local_year_df['DATE']).month):\n",
    "            #Get all samples for each month\n",
    "            KELIBIA = full_data[(pd.DatetimeIndex(full_data['DATE']).month== month) & (pd.DatetimeIndex(full_data['DATE']).year ==year)]\n",
    "            date = KELIBIA[['DATE']]\n",
    "            # get min, max, mean, std, 25%, 50% and 75% for each column\n",
    "            KELIBIA = KELIBIA.describe().drop(['count'],axis=0)\n",
    "            #convert the DF from N*M dimonssion to 1*(M*N)  dimonssion\n",
    "            KELIBIA = KELIBIA.unstack().to_frame().T\n",
    "            KELIBIA.columns = ['_'.join(column) for column in KELIBIA.columns]\n",
    "            #set the first date to meet as index \n",
    "            KELIBIA['DATE'] = date.iloc[0][0]\n",
    "            KELIBIA = KELIBIA.set_index(KELIBIA['DATE'])\n",
    "\n",
    "            new_data = new_data.append(KELIBIA)\n",
    "    new_data = new_data.drop(['DATE','APP_std','APP_min','APP_25%','APP_50%','APP_75%','APP_max'],axis=1)\n",
    "    new_data.rename(columns={'APP_mean':'APP'},inplace=True)\n",
    "\n",
    "    return new_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_features_targets = create_statistic_data(ts_features_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values: 0\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(original_data):\n",
    "    clean_data = original_data.copy()\n",
    "    clean_data.drop_duplicates(keep='first', inplace=True)\n",
    "    ### fill missing data \n",
    "    clean_data.interpolate(inplace=True)\n",
    "    clean_data.fillna(method='bfill',inplace=True)    \n",
    "    #print('missing values:',clean_data.isna().sum().sum())\n",
    "    return clean_data\n",
    "\n",
    "ts_features_targets = preprocessing(ts_features_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ts_features_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split & Scaling & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train & test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = ts_features_targets.drop('APP', axis=1)\n",
    "y = ts_features_targets[['APP']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling and transform data\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "def data_scaling_transform(X_train, X_test, y_train, y_test):\n",
    "    ### transform input variables\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    ## transform target variables\n",
    "    transformer = PowerTransformer().fit(y_train)\n",
    "    y_train = transformer.transform(y_train)\n",
    "    y_test = transformer.transform(y_test)     \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_scaling_transform(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(LinearRegression())\n",
    "    models.append(HuberRegressor())\n",
    "    models.append(Ridge())\n",
    "    models.append(Lasso(max_iter=1500))\n",
    "    models.append(KNeighborsRegressor(n_neighbors=3))\n",
    "    models.append(SVR())\n",
    "    models.append(DecisionTreeRegressor())\n",
    "    models.append(ExtraTreeRegressor())\n",
    "    models.append(BaggingRegressor(base_estimator=DecisionTreeRegressor()))\n",
    "    models.append(RandomForestRegressor())\n",
    "    models.append(ExtraTreesRegressor())\n",
    "    models.append(AdaBoostRegressor(random_state=seed))\n",
    "    models.append(GradientBoostingRegressor(random_state=seed))\n",
    "    models.append(XGBRegressor(objective ='reg:squarederror'))\n",
    "    models.append(MLPRegressor())\n",
    "    return models\n",
    "\n",
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, y, models):\n",
    "    meta_X, meta_y = list(), list()\n",
    "    # define split of data\n",
    "    splits = 5\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    # enumerate splits\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        splits-=1\n",
    "        fold_yhats = list()\n",
    "        # get data\n",
    "        train_X, test_X = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]\n",
    "        meta_y.extend(test_y)\n",
    "        # fit and make predictions with each sub-model\n",
    "        for model in models:\n",
    "            #print((train_X.dtypes), (train_y.dtypes))            \n",
    "            model.fit(train_X, train_y)\n",
    "            yhat = model.predict(test_X)\n",
    "            # store columns\n",
    "            fold_yhats.append(yhat.reshape(len(yhat),1))\n",
    "        # store fold yhats as columns\n",
    "        meta_X.append(hstack(fold_yhats))\n",
    "    return vstack(meta_X), asarray(meta_y)\n",
    "\n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, y, models):\n",
    "    for index, model in enumerate(models):\n",
    "        models[index] = model.fit(X, y)\n",
    "    return models   \n",
    "\n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, y, models):\n",
    "    df  = pd.DataFrame()\n",
    "    for model in models:\n",
    "        yhat = model.predict(X)\n",
    "        mse = mean_squared_error(y, yhat)\n",
    "        #print('%s: RMSE: %.3f , R2: %3f '  % (model.__class__.__name__, sqrt(mse), r2_score(y, yhat)))\n",
    "        df = df.append(pd.DataFrame([[model.__class__.__name__, sqrt(mse), r2_score(y, yhat)]],columns=['model_name','RMSE','R2']))\n",
    "    return df\n",
    "\n",
    "def super_learner_predictions(X, models, meta_model):\n",
    "    meta_X = list()    \n",
    "    for model in models:\n",
    "        yhat = model.predict(X)\n",
    "        meta_X.append(yhat.reshape(len(yhat),1))\n",
    "    meta_X = hstack(meta_X)\n",
    "    # predict\n",
    "    return meta_model.predict(meta_X)\n",
    "    \n",
    "def train(X, X_val, y, y_val):\n",
    "   \n",
    "    # get models\n",
    "    models = get_models()\n",
    "    # get out of fold predictions\n",
    "    start_time = time.time() \n",
    "    meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
    "    #print('Meta ', len(meta_X), len(meta_y))\n",
    "    # fit base models\n",
    "    models = fit_base_models(X, y, models)\n",
    "    # fit the meta model\n",
    "    meta_model = fit_meta_model(meta_X, meta_y)\n",
    "    # evaluate base models\n",
    "    res = evaluate_models(X_val, y_val, models)\n",
    "    # evaluate meta model\n",
    "    yhat = super_learner_predictions(X_val, models, meta_model)\n",
    "\n",
    "    #res  = pd.DataFrame()\n",
    "    res = res.append(pd.DataFrame([[meta_model.__class__.__name__, sqrt(mean_squared_error(y_val, yhat)), r2_score(y_val, yhat)]],columns=['model_name','RMSE','R2']))\n",
    "    #print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))\n",
    "    #print('Super Learner: R2 %.3f' % (r2_score(y_val, yhat)))\n",
    "    return res, models, meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, models, meta_model):\n",
    "    yhat = super_learner_predictions(X, models, meta_model)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing values: 0\n",
      "0.6599871111457973\n"
     ]
    }
   ],
   "source": [
    "#ts_features_targets = pd.read_excel(CLEANED_DATA)\n",
    "ts_features_targets = pd.read_csv(cleaned_data_with_AEGMM)\n",
    "ts_features_targets = ts_features_targets.set_index(ts_features_targets['DATE']).drop(['DATE'],axis = 1)\n",
    "\n",
    "\n",
    "ts_features_targets = create_statistic_data(ts_features_targets)\n",
    "\n",
    "\n",
    "ts_features_targets = preprocessing(ts_features_targets)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_scaling_transform(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "res, models, meta_model = train(X_train.reset_index().drop(['DATE'], axis = 1), X_test.reset_index().drop(['DATE'], axis = 1), pd.DataFrame(y_train,columns=['app']).app, pd.DataFrame(y_test,columns=['app']).app)\n",
    "\n",
    "print(res.R2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
